{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c927e-4dd8-4fc3-b411-24c3c0eba93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3c6f1-a4a9-484b-b1e0-63b96c407f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, in_channels, use_bn = False):\n",
    "        super().__init__()\n",
    "        self.use_bn = use_bn\n",
    "        if self.use_bn:\n",
    "            self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "            self.bn2 = nn.BatchNorm2d(in_channels)    \n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU() # maybe not be required\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = x\n",
    "        if self.use_bn:\n",
    "            out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv1(out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv2(out)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class ResidualPipeline(nn.Module):\n",
    "    def __init__(self, in_channels, n_units, use_bn = False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        layers = []\n",
    "        for i in range(n_units):\n",
    "            layers.append(ResidualUnit(64, use_bn))\n",
    "        self.resnet_stack = nn.Sequential(*layers)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.resnet_stack(x)\n",
    "        x = self.relu(x) # not there in original paper\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class ExternalEncoder(nn.Module):\n",
    "    def __init__(self, ext_dim, map_dim):\n",
    "        super().__init__()\n",
    "        self.map_dim = map_dim\n",
    "        self.fcc1 = nn.Linear(ext_dim, 60, bias = True)\n",
    "        self.fcc2 = nn.Linear(60, np.prod(map_dim), bias = True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU() # maybe not be required\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fcc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fcc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x.view(-1, *self.map_dim)\n",
    "    \n",
    "class STResnet(nn.Module):\n",
    "    def __init__(self, c_channel, p_channel, t_channel, n_residual_units, ext_dim, map_dim, use_bn = False):\n",
    "        super().__init__()\n",
    "        self.e_pipe = ExternalEncoder(ext_dim, map_dim)\n",
    "        self.c_pipe = ResidualPipeline(c_channel, n_residual_units, use_bn)\n",
    "        self.p_pipe = ResidualPipeline(p_channel, n_residual_units, use_bn)\n",
    "        self.t_pipe = ResidualPipeline(t_channel, n_residual_units, use_bn)\n",
    "        # 1 dimension for batch processing, this class cannot process unbatched data\n",
    "        self.w_c = nn.Parameter(torch.randn(1, *map_dim))\n",
    "        self.w_p = nn.Parameter(torch.randn(1, *map_dim))\n",
    "        self.w_t = nn.Parameter(torch.randn(1, *map_dim))\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x_c, x_p, x_t, x_e):\n",
    "        y_e = self.e_pipe(x_e)\n",
    "        y_c = self.c_pipe(x_c)\n",
    "        y_p = self.p_pipe(x_p)\n",
    "        y_t = self.t_pipe(x_t)\n",
    "        # Fusion: Eliment wise product (Hadamard Product)\n",
    "        y = self.w_c*y_c + self.w_p*y_p + self.w_t*y_t\n",
    "        y = self.tanh(y+y_e)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a510d0-013c-4aff-ab42-b2f3320c5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler, random_split\n",
    "from TaxiBJ import TaxiBJDataset\n",
    "n_closeness = 4\n",
    "n_period = 1\n",
    "n_trend = 1\n",
    "flow_ch = 2 # Inflow and Outflow\n",
    "n_residual_units = 12\n",
    "map_dim = (flow_ch, 32, 32) # Specific to dataset\n",
    "ext_dim = 27\n",
    "taxibj_dataset_train = TaxiBJDataset('./Datasets/TaxiBJ/',n_closeness,n_period,n_trend,0.8,True)\n",
    "taxibj_dataset_test = TaxiBJDataset('./Datasets/TaxiBJ/',n_closeness,n_period,n_trend,0.8,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b469c-ed4f-4772-b5fe-83bbcf460614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "mmn = taxibj_dataset_train.mmn\n",
    "\n",
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    n_batches = 0\n",
    "    for x_c, x_p, x_t, y in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_c, x_p, x_t)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    return train_loss/n_batches\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    n_batches = 0\n",
    "    for x_c, x_p, x_t, y in dataloader:\n",
    "        output = model(x_c, x_p, x_t)\n",
    "        loss = loss_fn(output, y)\n",
    "        valid_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    return valid_loss/n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc1ae-8f39-42a4-8f1b-a4ad2bd88d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "train_size = int(0.9 * len(taxibj_dataset_train))\n",
    "valid_size = len(taxibj_dataset_train) - train_size\n",
    "train_idx, valid_idx = random_split(range(len(taxibj_dataset_train)), [train_size, valid_size])\n",
    "train_loader = DataLoader(taxibj_dataset_train, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n",
    "valid_loader = DataLoader(taxibj_dataset_train, batch_size=batch_size, sampler=SubsetRandomSampler(valid_idx))\n",
    "\n",
    "model = model = STResnet(n_closeness*flow_ch,n_period*flow_ch,n_trend*flow_ch,n_residual_units,ext_dim,map_dim,True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "history = {'train_loss': [], 'valid_loss': []}\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss=train_epoch(model,train_loader,loss_fun,optimizer)\n",
    "    valid_loss=valid_epoch(model,valid_loader,loss_fun)\n",
    "    print(\"Epoch:{}/{} | Training Loss: {} | Validation Loss: {}\".format(epoch + 1, num_epochs, train_loss, valid_loss))\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['valid_loss'].append(valid_loss)\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# splits = KFold(n_splits=10,shuffle=True)\n",
    "# foldperf={}\n",
    "# least_loss_yet = 2 # maximum loss we can expect, can be any large number\n",
    "# for fold, (train_idx,valid_idx) in enumerate(splits.split(np.arange(len(taxibj_dataset_train)))):\n",
    "#     print('Fold {}'.format(fold + 1))\n",
    "#     train_loader = DataLoader(taxibj_dataset_train, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n",
    "#     valid_loader = DataLoader(taxibj_dataset_train, batch_size=batch_size, sampler=SubsetRandomSampler(valid_idx))\n",
    "\n",
    "#     model = model = STResnet(n_closeness*flow_ch,n_period*flow_ch,n_trend*flow_ch,n_residual_units,ext_dim,map_dim,True).to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#     loss_fun = nn.MSELoss()\n",
    "\n",
    "#     history = {'train_loss': [], 'valid_loss': []}\n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_loss=train_epoch(model,train_loader,loss_fun,optimizer)\n",
    "#         valid_loss=valid_epoch(model,valid_loader,loss_fun)\n",
    "#         print(\"Epoch:{}/{} | Training Loss: {} | Validation Loss: {}\".format(epoch + 1, num_epochs, train_loss, valid_loss))\n",
    "#         history['train_loss'].append(train_loss)\n",
    "#         history['valid_loss'].append(valid_loss)\n",
    "#         if valid_loss < least_loss_yet:\n",
    "#             print('Saving a better model!')\n",
    "#             torch.save(model,'checkpoints/k_fold_stresnet.pt')\n",
    "#             least_loss_yet = valid_loss\n",
    "        \n",
    "#     foldperf['fold{}'.format(fold+1)] = history     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720aa07-7750-448e-862e-f0bc66274df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "test_loader = DataLoader(taxibj_dataset_test, batch_size=batch_size)\n",
    "test_loss = 0.0\n",
    "model = torch.load('checkpoints/k_fold_stresnet.pt')\n",
    "model.eval()\n",
    "n_batches = 0\n",
    "for x_c, x_p, x_t, y in test_loader:\n",
    "    output = mmn.inverse_transform(model(x_c, x_p, x_t))\n",
    "    loss = loss_fun(output, mmn.inverse_transform(y))\n",
    "    test_loss += loss.item()\n",
    "    n_batches += 1\n",
    "rmse_loss = math.sqrt(test_loss/n_batches)\n",
    "print('Root Mean Square Loss: ', rmse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc62552-aa01-47fc-8ce2-f3a12d6f3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_pred = np.zeros((len(taxibj_dataset_test),*map_dim))\n",
    "flow_org = np.zeros((len(taxibj_dataset_test),*map_dim))\n",
    "for i in range(len(taxibj_dataset_test)):\n",
    "    x_c, x_p, x_t, y = taxibj_dataset_test[i]\n",
    "    x_c = torch.unsqueeze(x_c, 0)\n",
    "    x_p = torch.unsqueeze(x_p, 0)\n",
    "    x_t = torch.unsqueeze(x_t, 0)    \n",
    "    y_pred = model(x_c, x_p, x_t)\n",
    "    flow_org[i,:,:,:] = mmn.inverse_transform(y).cpu().detach().numpy()[:,:,:]\n",
    "    flow_pred[i,:,:,:] = mmn.inverse_transform(y_pred).cpu().detach().numpy()[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c13254-5bad-4685-88ec-cc0d36a7356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x, y = 10, 3\n",
    "\n",
    "fig = plt.figure()\n",
    "in_flow_org = flow_org[24*7*0:24*7*1,0,x,y]\n",
    "in_flow_pred = flow_pred[24*7*0:24*7*1,0,x,y]\n",
    "t = np.linspace(1, 24*7*1, num=24*7*1)\n",
    "plt.plot(t,in_flow_org,label='Inflow Org')\n",
    "plt.plot(t,in_flow_pred,label='Inflow Pred')\n",
    "plt.xticks(np.linspace(1+19, 168+19, num=8),['Tue','Wed','Thr','Fri','Sat','Sun','Mon',''])\n",
    "plt.title('Inflow in a busy NYC area from Aug 25 - 7pm to Sep 1 - 7pm, 2014')\n",
    "plt.ylabel('Flow')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "out_flow_org = flow_org[24*7*0:24*7*1,1,x,y]\n",
    "out_flow_pred = flow_pred[24*7*0:24*7*1,1,x,y]\n",
    "t = np.linspace(1, 24*7*1, num=24*7*1)\n",
    "plt.plot(t,out_flow_org,label='Outflow Org')\n",
    "plt.plot(t,out_flow_pred,label='Outflow Pred')\n",
    "plt.xticks(np.linspace(1+19, 168+19, num=8),['Tue','Wed','Thr','Fri','Sat','Sun','Mon',''])\n",
    "plt.title('Outflow in a busy NYC area from Aug 25 - 7pm to Sep 1 - 7pm, 2014')\n",
    "plt.ylabel('Flow')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
